{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Fc-cAAWOp4k"
   },
   "source": [
    "# MALIS Lab Session 2 - Fall 2021\n",
    "## Due Date: Dec 3 23h59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyM-uj_BOp4m"
   },
   "source": [
    "**Group :**\n",
    "\n",
    "**Name SURNAME, Name SURNAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AQtwDCKOp4n"
   },
   "source": [
    "The aim of this lab is to practice with Neural Networks (Multi-Layer Perceptrons) via simple classification experiments and the implementation of the feedforward and backpropagation procedures.\n",
    "\n",
    "#### Learning goals\n",
    "After this lab, you should be able to:\n",
    "1. Be familiar with the elements required to define the architecture of a neural network (NN).\n",
    "2. Understand the two procedures needed to train a neural network: feedforward and backpropagation\n",
    "3. Understand the role of the learning rate and the number of iterations in the training process of a NN and how it these can affect performance.\n",
    "\n",
    "#### Instructions:\n",
    "Experiments should be made by groups of two students. Each group should produce a Jupyter Notebook with all their results and comments. We strongly encourage the addition of plots and visual representation to the report, bearing in mind that comments on the graphical data are still necessary. Code for adding images to your notebook: ```<img src=\"path/to/image.png\" />```. <Plateforme soumission des notebooks>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZKs_0AtOp4o"
   },
   "source": [
    "<h2>Introduction</h2>\n",
    "There are three parts to this lab session. \n",
    "\n",
    "1. A \"theoretical\" part: Given a set of training examples you have to decide on the architecture of the feed-forward neural network such as; number of layers, number of neuron per layers and finally the values of the weights. \n",
    "\n",
    "2. A \"programming\" part: Given the skeleton of the Python code of an MLP simulator, implement the missing functions (feedforward and backpropagation procedures). \n",
    "\n",
    "3. An \"experimental\" part: Having completed the implementation of the MLP simulator, the final step consist on training the network and testing it.\n",
    "\n",
    "<h2>Part 1: Design a neural network</h2>\n",
    "The aim of this part is to get a better understanding of the basics of Neural Networks construction. A number of sample points on a 128 by 128 grid have been assigned one out of three colors (red, green or blue). You should build a Neural Network with two inputs and three outputs which provides the exact coloring for these points. The problem can be visualized in the following figure: \n",
    "\n",
    "<img src=\"data_set.jpg\" />\n",
    "\n",
    "The file set30.x1x2rgb (in .\\data\\) contains the data corresponding to the problem defined above. The visual representation of the problem (above figure) is stored in data_set.jpg.\n",
    "\n",
    "The problem:\n",
    "\n",
    "Pairs of x1 and x2 coordinates (both ranging between 0 and 127) are associated with a specific color: \n",
    "\n",
    "* Red: output 1 0 0, \n",
    "* Green: output 0 1 0, \n",
    "* Blue: output 0 0 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question :** How many linear separations would be needed to perfectly separate the data points? Using the visual representation, give some appropriate equations of such linear separations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer :** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the network is to correctly determine for any given (x1, x2) coordinate pair the corresponding color. \n",
    "Using the equations you proposed before, along with your lectures knowledge, your task is to <b>manually define a Neural Network which performs this task perfectly</b>. There is no need for programming or iterative training. The transfer function is assumed to be the step function: \n",
    "\n",
    "$f(t) = (t > 0)$ (it is equal to 1 if t is positive, 0 otherwise). \n",
    "\n",
    "Of course, it is your task to define the number of layers, the number of neurons per layer, and the exact values for the weights. \n",
    "\n",
    "<i>Hint: We may remember the XOR problem and how it was solved. Think also how many lines you need to create areas with only elements of that color and if the color is below or above that color.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kkmqn1bnOp4p"
   },
   "source": [
    "### Your answer :\n",
    "\n",
    "( *Fill in the correct data and fill the respective weigths and biases* )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "raw",
    "id": "rMV9r4dROp4p"
   },
   "source": [
    "Input layer:  2 units,  x1   x2\n",
    "\n",
    "First hidden layer:\n",
    "    n1 neurons:\n",
    "    neuron 1: w11 =  <- weight from input x1 to neuron 1\n",
    "              w12 =  <- weight from input x2 to neuron 1\n",
    "              b1  =  <- bias of neuron 1 of the first hidden layer\n",
    "    neuron 2: w21 = \n",
    "              w22 = \n",
    "              b2  = \n",
    "    neuron 3: w31 = \n",
    "              w32 = \n",
    "              b2  = \n",
    "    ...\n",
    "              \n",
    "Second hidden layer:\n",
    "    n2 neurons:\n",
    "    neuron 1: w11 =  <- weight FROM neuron 1 in first hidden layer TO neuron 1 in second hidden layer\n",
    "              w12 =  <- weight FROM neuron 2 in first hidden layer TO neuron 1 in second hidden layer\n",
    "              ...\n",
    "              b1  =  <- bias of neuron 1 of the second hidden layer\n",
    "    neuron 2: w21 = \n",
    "              w22 =\n",
    "              ...\n",
    "              b2  =              \n",
    "    ...\n",
    "    \n",
    "...\n",
    "\n",
    "output layer:\n",
    "    n_out neurons:\n",
    "    neuron 1: w11 = \n",
    "              w12 = \n",
    "              ...\n",
    "              b1  = \n",
    "    neuron 2: w21 = \n",
    "              w22 =\n",
    "              ...\n",
    "              b2  = \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cy0DpBiOp4q"
   },
   "source": [
    "#### Test with the data\n",
    "Test with the data in ./data/set30.x1x2rgb, complete the code below with your values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "print_solutions=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "dKoW4efyOp4r",
    "outputId": "a98ab8c3-5587-4464-ec7a-f6455a1797a6"
   },
   "outputs": [],
   "source": [
    "rgb_df=pd.read_csv('colors.csv')\n",
    "\n",
    "x=np.array(rgb_df[['x1','x2']].copy())\n",
    "y=np.array(rgb_df[['y1','y2','y3']].copy())\n",
    "\n",
    "######################### YOUR VALUES HERE ####################\n",
    "\n",
    "# weight input -> 1st hidden layer\n",
    "W_1 =numpy.array([[w11, w12 ...], # to n1 of first hidden layer\n",
    "      [w21, w22, ...], # to n2 of first hidden layer\n",
    "      [...]])\n",
    "# bias 1st hidden layer\n",
    "b_1 = numpy.array([b1, b2, ...]).T \n",
    "\n",
    "\n",
    "# weigth 1st hidden layer -> 2nd hidden layer\n",
    "W_2 =numpy.array([[w11, w12 ...], # to n1 of first hidden layer\n",
    "      [w21, w22, ...], # to n2 of first hidden layer\n",
    "      [...]])\n",
    "# bias 2nd hidden layer\n",
    "b_2 = numpy.array([b1, b2, ...]).T \n",
    "\n",
    "\n",
    "... # add other layers if necessary\n",
    "\n",
    "\n",
    "# nth hidden layer -> output layer\n",
    "W_out =numpy.array([[w11, w12 ...], # to n1 of first hidden layer\n",
    "      [w21, w22, ...], # to n2 of first hidden layer\n",
    "      [...]])\n",
    "b_out = numpy.array([b1, b2, ...]).T \n",
    "\n",
    "\n",
    "x1 = (b_1 + x @ W_1.T)>0 # @ is the matrix product\n",
    "x2 = (b_2 + x1 @ W_2.T)>0\n",
    "...\n",
    "y_hat = (b_out + xn @ W_out.T)>0\n",
    "\n",
    "######################### END YOUR VALUES ######################\n",
    "\n",
    "# You should get 100 %\n",
    "print('accuracy : ' , 100*np.sum(y_hat==y)/(3*len(y)),'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "id": "3bjVtxx6Op4w",
    "outputId": "686a0d8f-9960-4063-df83-1cdac632d6a7"
   },
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/part1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FptDCtcaOp40"
   },
   "source": [
    "## Part 2: Implementation of a Neural Network / Multi-Layer Perceptrons\n",
    "\n",
    "In order to implement a neural network, firstly we have to implement the basic blocks, then combine them.\n",
    "\n",
    "1. **Initialization of parameters**\n",
    "    1. need as parameters the number of neurons in input, hidden layer1, hidden layer2, ..., output\n",
    "    2. random initialization of the parameters\n",
    "2. **Activation functions**\n",
    "    1. define the function sigmoid and sigmoid_derivative\n",
    "2. **Forward function**\n",
    "    1. using inputs, weights, activation functions compute y_hat\n",
    "3. **Loss function**\n",
    "    1. given as input the true y and y_hat, compute the loss\n",
    "4. **Accuracy**\n",
    "    1. given as input the true y and y_hat, compute the accuracy\n",
    "5. **Backward function**\n",
    "    1. gradient computations from last layer to first layer\n",
    "    2. update of parameters (weights,...)\n",
    "6. **Training**\n",
    "    1. needs as parameters the inputs and corresponding outputs,the learning rate, the number of epochs and the parameter verbose\n",
    "    2. repeat for the number of epochs:\n",
    "        1. shuffle the inputs\n",
    "        2. for each input : forward, loss, backward\n",
    "        3. loss and save it and if verbose==True print it\n",
    "        4. accuracy and save it and if verbose==True print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a look to the file **NeuralNetwork.py** and then return to the notebook to implement the missing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import logistic\n",
    "# from the file NeuralNetwork.py we import the Multi-Layer Perceptron\n",
    "from NeuralNetwork import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Activation functions**\n",
    "\n",
    "In MLP there is the function sigmoid. Using MPL.sigmoid(), implement its derivative function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(a) :\n",
    "    '''\n",
    "    Derivative of sigmoid activation function. It can work with single inputs or vectors or matrices.\n",
    "    Return the sigmoid derivative of a\n",
    "    '''\n",
    "    ################# YOUR CODE HERE ####################\n",
    "\n",
    "    \n",
    "    ################ END OF YOUR CODE HERE ##############\n",
    "    \n",
    "MLP.d_sigmoid=d_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_solutions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-97ca0c49353b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# do NOT modify this cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mprint_solutions\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./solutions/d_sigmoid.py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_solutions' is not defined"
     ]
    }
   ],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/d_sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Feedforward function**\n",
    "\n",
    "Write a function which performs the forward operation from input to output layer. Remember you have len(self.layers) number of layers and each layer has its own parameters:\n",
    "1. self.layer[0].W are the weights between input and 1st hidden layer\n",
    "2. self.layer[0].b are the biases of the 1st hidden layer\n",
    "3. ...\n",
    "\n",
    "Each layer has as activation function the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x) :\n",
    "    '''\n",
    "    Forward function. From input layer to output layer. Input can handle 1D or 2D inputs.\n",
    "\n",
    "    INPUTS:\n",
    "    - x : numpy array of size NxD, where N is the number of samples, D is the number of input dimensions referred as n_input before\n",
    "\n",
    "    OUTPUTS:\n",
    "    - y_hat : numpy array of size NxC, where C is the number of classes\n",
    "    '''\n",
    "    ################# YOUR CODE HERE ####################\n",
    "\n",
    "\n",
    "    ################ END OF YOUR CODE HERE ##############\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "MLP.forward=forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/forward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Loss function**\n",
    "\n",
    "Compute the mean square loss between y_hat and y\n",
    "$$L = \\frac{1}{N}\\sum_{n=0}^{N-1} \\left(\\frac{1}{2} \\sum_{k=0}^{C-1} (\\hat{y}_{k,n} - y_{k,n})^2\\right)$$\n",
    "with $k$ be the class, and $n$ the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y) :\n",
    "    '''\n",
    "    Compute the loss between y_hat and y! they can be 1D or 2D arrays!\n",
    "\n",
    "    INPUTS:\n",
    "    - y_hat : numpy array of size NxC, N number of samples, C number of classes. It contains the estimated values of y\n",
    "    - y : numpy array of size NxC with one 1 in each row, corresponding to the correct class for that sample\n",
    "\n",
    "    OUTPUTS:\n",
    "    - L : MSE loss\n",
    "    '''\n",
    "    ################# YOUR CODE HERE ####################\n",
    "\n",
    "    ################ END OF YOUR CODE HERE ##############\n",
    "\n",
    "    return L\n",
    "\n",
    "MLP.loss=loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Accuracy**\n",
    "\n",
    "Compute the accuracy counting how many y_hat are equal to y over the total number of N samples. Remember the accuracy is a value in [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y) :\n",
    "    '''\n",
    "    Compute the accuracy between y_hat and y\n",
    "\n",
    "    INPUTS:\n",
    "    - y_hat : numpy array of size NxC, C number of classes. It contains the estimated values of y\n",
    "    - y : numpy array of size NxC with correct values of y\n",
    "\n",
    "    OUTPUTS:\n",
    "    - acc : the accuracy value between 0 and 1\n",
    "    '''\n",
    "    ################# YOUR CODE HERE ####################\n",
    "\n",
    "\n",
    "    ################ END OF YOUR CODE HERE ##############\n",
    "\n",
    "    return acc\n",
    "\n",
    "MLP.accuracy=accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/accuracy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Backpropagation**\n",
    "\n",
    "You can perform it in 2 ways. The first neuron by neuron as done during the lecture, or you can use matrices as explained here. The result are equivalent. A full matrix derivation can be found here http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^L}=\\frac{\\partial L}{\\partial \\hat{y}}=\\delta_L=\\hat{y}-y=z^L-y$$\n",
    "\n",
    "**Important** : $\\hat{y}$ and $y$ are row vectors 1$\\times$C, with C the number of classes, while $a$, $z$ and $b$ are row vectors too in this notation. So are the derivatives w.r.t. them. $W^l$ is instead a matrix $n^l \\times n^{l-1}$, where $n^l$ is the number of neurons at layer $l$, while  $n^{l-1}$ is the number of neurons at layer $l-1$\n",
    "\n",
    "**Generic layer $l$**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^l}=\\delta_l=\\frac{\\partial L}{\\partial a^{l+1}}\\ W^{l+1}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^l}=\\delta_l \\odot \\sigma'(a^l)$$\n",
    "\n",
    "$\\odot$ is the Hadamard product or elementwise multiplication\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^l}=\\frac{\\partial L}{\\partial a^l}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^l}=\\left(\\frac{\\partial L}{\\partial a^l}\\right)^T\\ (z^{l-1})$$\n",
    "\n",
    "with $z^{-1}$, the z at layer -1 is $x$ the input vector of size 1$\\times$D, where D is the number of features\n",
    "\n",
    "After all computations, remember to compute the update of the gradients using the learning rate $\\eta$\n",
    "\n",
    "$$W^l_{new}=W^l-\\eta \\frac{\\partial L}{\\partial W^l}$$\n",
    "\n",
    "$$b^l_{new}=b^l-\\eta \\frac{\\partial L}{\\partial b^l}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self,x,y,y_hat,learning_rate) :\n",
    "    '''\n",
    "    Backpropagate the error from last layer to input layer and then update the parameters\n",
    "\n",
    "    INPUTS:\n",
    "    - y_hat : numpy array of size NxC, C number of classes. It contains the estimated values of y\n",
    "    -y : numpy array of size NxC with correct values of y\n",
    "\n",
    "    OUTPUTS: (compute the error at the different levels and for each layer)\n",
    "    - d_a\n",
    "    - d_z\n",
    "    - delta_L\n",
    "    - delta_l\n",
    "    - d_W\n",
    "    - d_b\n",
    "    '''\n",
    "# compute gradients\n",
    "\n",
    "    ################# YOUR CODE HERE ####################\n",
    "\n",
    "    \n",
    "    ################ END OF YOUR CODE HERE ##############\n",
    "\n",
    "# apply gradients\n",
    "    # just one for loop passing through all layers is sufficient\n",
    "    # apply the gradients only to self.layer[i].b and self.layer[i].W\n",
    "\n",
    "    ################# YOUR CODE HERE ####################\n",
    "\n",
    "    \n",
    "    ################ END OF YOUR CODE HERE ##############\n",
    "    \n",
    "MLP.backpropagation=backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOTD_KvkOp41"
   },
   "outputs": [],
   "source": [
    "# do NOT modify this cell\n",
    "if print_solutions==True :\n",
    "    %load ./solutions/backpropagation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Training and Recall experiments</h2>\n",
    "\n",
    "Train the network using the iris dataset (https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    "* 4 features for every input\n",
    "* 3 possible labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EsfSDYUpWRs"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "shuffle=np.random.permutation(range(len(iris.data)))\n",
    "X = np.array(iris.data)[shuffle,:]\n",
    "y = np.eye(3)[iris.target,:]\n",
    "y=y[shuffle,:]\n",
    "\n",
    "x_train=X[0:120,:]\n",
    "y_train=y[0:120,:]\n",
    "x_test=X[120:,:]\n",
    "y_test=y[120:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network (NN) parameters\n",
    "epochs=50\n",
    "learning_rate=0.01\n",
    "verbose=True\n",
    "print_every_k=10\n",
    "\n",
    "# Initialization of the NN\n",
    "NN1 = MLP([4, 10, 3])\n",
    "print('TRAINING')\n",
    "# Training\n",
    "NN1.training(x_train,y_train,learning_rate,epochs,verbose,print_every_k)\n",
    "# Compute the training loss and accuracy after having completer the training\n",
    "y_hat=NN1.forward(x_train)\n",
    "print('final : loss = %.3e , accuracy = %.2f %%'%(MLP.loss(y_hat,y_train),100*MLP.accuracy(y_hat,y_train)))\n",
    "\n",
    "# Test\n",
    "print('\\nTEST')\n",
    "y_hat=NN1.forward(x_test)\n",
    "print('loss = %.3e , accuracy = %.2f %%\\n'%(MLP.loss(y_hat,y_test),100*MLP.accuracy(y_hat,y_test)))\n",
    "\n",
    "plt.plot(list(range(epochs)),NN1.losses,c='r',marker='o',ls='--');\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss value\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(range(epochs)),NN1.accuracies,c='g',marker='o',ls='--');\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous cell as example, try to change the parameters in order to obtain a training accuracy above 99% and a test accuracy above 95%.\n",
    "\n",
    "**Explain** the difference between the new parameters and the old ones. What was missing in the old ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab2_neural_networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
